{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32788 train samples loaded\n",
      "order_id                               int64\n",
      "total_cents                            int64\n",
      "subtotal_cents                         int64\n",
      "discount                               int64\n",
      "discount_prc                         float64\n",
      "cash_flg                               int64\n",
      "auto_flg                               int64\n",
      "dow                                    int64\n",
      "hot                                    int64\n",
      "additionals_num                        int64\n",
      "day_slot_flg                           int64\n",
      "has_comment                            int64\n",
      "subscription_flg                       int64\n",
      "lat                                  float64\n",
      "lng                                  float64\n",
      "creation_start_hours                   int64\n",
      "cancel                                 int64\n",
      "paid_all                               int64\n",
      "canceled_all_nt                        int64\n",
      "canceled_all_nt_share                float64\n",
      "paid_16d                               int64\n",
      "paid_30d                               int64\n",
      "paid_60d                               int64\n",
      "canceled_16d_nt                        int64\n",
      "canceled_30d_nt                        int64\n",
      "canceled_60d_nt                        int64\n",
      "shifts_num                           float64\n",
      "canceled_onetime_nt                    int64\n",
      "days_since_last_cancel_all_nt        float64\n",
      "days_since_last_cancel_onetime_nt    float64\n",
      "days_since_last_paid_order           float64\n",
      "dow_paid                               int64\n",
      "dow_canceled                           int64\n",
      "dow_paid_share                       float64\n",
      "dow_all_paid_share                   float64\n",
      "segments                              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "features = pd.read_csv('train/aidata.csv', header = 0, dtype={'segments': np.str})\n",
    "X = features.drop(['order_id','cancel', 'dow', 'segments'], axis=1)\n",
    "y = features['cancel']\n",
    "print('{0} train samples loaded'.format(len(X)))\n",
    "\n",
    "print(features.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = []\n",
    "\n",
    "for i in range(len(y)):\n",
    "    segments_tmp = str(X['segments'][i]).split('/')\n",
    "    for s in segments_tmp:\n",
    "        if s not in segments:\n",
    "            segments.append(s)\n",
    "\n",
    "#print(segments)\n",
    "\n",
    "dic_df = pd.DataFrame(columns=segments, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32788, 1221)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    segments_tmp = str(X['segments'][i]).split('/')\n",
    "    for s in segments_tmp:\n",
    "        try:\n",
    "            dic_df[s][i] = 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "print(dic_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32788, 1254)\n"
     ]
    }
   ],
   "source": [
    "#X = X.merge(aidata, how='left', on = 'tracking_id')\n",
    "\n",
    "X = pd.concat([X, dic_df], axis=1)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['segments'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32788, 1253)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def fill_na_zero(df, col_name):\n",
    "#    df[col_name] = df[col_name].fillna(df[col_name].mean())\n",
    "#    df = df.fillna(0)\n",
    "#    #df = df.fillna(df.mean())\n",
    "#    return df\n",
    "\n",
    "def fill_na_mean(df, col_names):\n",
    "    for c in col_names:\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "    return df\n",
    "\n",
    "def le_fit_transform(df, col_name):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[col_name])\n",
    "    df[col_name] = le.transform(df[col_name])\n",
    "    return df, le\n",
    "\n",
    "def le_transform(df, col_name, le):\n",
    "    df[col_name] = le.transform(df[col_name])\n",
    "    return df\n",
    "\n",
    "def ohc_fit_transform(df, col_name):\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(df[col_name].values.reshape(-1, 1))\n",
    "    encoded_col = ohe.transform(df[col_name].values.reshape(-1, 1))\n",
    "    #encoded_col = ohe.fit_transform(df[col_name].values.reshape(-1, 1))\n",
    "    tmp = pd.DataFrame(encoded_col, columns=[col_name + str(i) for i in range(encoded_col.shape[1])], index = df.index)\n",
    "    df = pd.concat([df, tmp], axis = 1)\n",
    "    df = df.drop([col_name], axis = 1)\n",
    "    return df, ohe\n",
    "\n",
    "def ohc_transform(df, col_name, ohe):\n",
    "    encoded_col = ohe.transform(df[col_name].values.reshape(-1, 1))\n",
    "    tmp = pd.DataFrame(encoded_col, columns=[col_name + str(i) for i in range(encoded_col.shape[1])], index = df.index)\n",
    "    df = pd.concat([df, tmp], axis = 1)\n",
    "    df = df.drop([col_name], axis = 1)\n",
    "    return df\n",
    "\n",
    "def exponentiation(df, cols):\n",
    "    for c in cols:\n",
    "        df[c + '_sq'] = df[c]**2\n",
    "        df[c + '_sqrt'] = np.sqrt(df[c])\n",
    "    return df\n",
    "        \n",
    "\n",
    " \n",
    "X = fill_na_mean(X, ['shifts_num', 'dow_paid_share'])\n",
    "X = X.fillna(0)\n",
    "\n",
    "\n",
    "#X, le_type = le_fit_transform(X, 'type')\n",
    "#X, ohc_type = ohc_fit_transform(X, 'type')\n",
    "\n",
    "#X, le_start_hour = le_fit_transform(X, 'start_hour')\n",
    "#X, ohc_le_start_hour = ohc_fit_transform(X, 'start_hour')\n",
    "\n",
    "#X, le_creation_mean = le_fit_transform(X, 'creation_mean')\n",
    "#X, ohc_creation_mean = ohc_fit_transform(X, 'creation_mean')\n",
    "\n",
    "#X, le_payment_type = le_fit_transform(X, 'payment_type')\n",
    "#X, ohc_payment_type = ohc_fit_transform(X, 'payment_type')\n",
    "\n",
    "# X = exponentiation(X, ['paid_all','canceled_all_nt','paid_16d', 'paid_30d', 'paid_60d', 'canceled_16d_nt','canceled_30d_nt', 'canceled_60d_nt', 'dow_paid', 'dow_canceled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "tmp = X.sum(axis=0)[X.sum(axis=0) >100]\n",
    "# print(tmp)\n",
    "#for t in tmp:\n",
    "#    if t == 0:\n",
    "#        print(t)\n",
    "del_list = []\n",
    "for c in X.columns:\n",
    "    if X[c].sum(axis=0) < 100:\n",
    "        del_list.append(c)\n",
    "        \n",
    "#for c in X.columns:\n",
    "#    if X[c].sum(axis=0) > 800:\n",
    "#        del_list.append(c)\n",
    " \n",
    "print(len(del_list))\n",
    "X = X.drop(del_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient boosting fitting:\n",
      "Trees 10, Depth 8, Learning Rate 0.2, Time to fit: 0:00:08.536340, ROC-AUC: 0.8313103294854534\n",
      "Trees 10, Depth 12, Learning Rate 0.2, Time to fit: 0:00:33.581750, ROC-AUC: 0.8363150981108888\n",
      "Trees 10, Depth 16, Learning Rate 0.2, Time to fit: 0:01:40.018537, ROC-AUC: 0.8248618428293403\n",
      "Trees 10, Depth 20, Learning Rate 0.2, Time to fit: 0:01:40.499153, ROC-AUC: 0.7964070930406068\n",
      "Trees 20, Depth 8, Learning Rate 0.2, Time to fit: 0:00:17.752863, ROC-AUC: 0.8393904115914351\n",
      "Trees 20, Depth 12, Learning Rate 0.2, Time to fit: 0:01:10.039264, ROC-AUC: 0.8482884635838007\n",
      "Trees 20, Depth 16, Learning Rate 0.2, Time to fit: 0:02:56.001169, ROC-AUC: 0.8383835024019454\n",
      "Trees 20, Depth 20, Learning Rate 0.2, Time to fit: 0:04:43.019465, ROC-AUC: 0.812052091834126\n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "kf = KFold(len(X), folds, shuffle=True, random_state=42)\n",
    "trees = [10, 20]\n",
    "depths = [8, 12, 16, 20]\n",
    "rates = [0.2]\n",
    "\n",
    "\n",
    "print()\n",
    "print('Gradient boosting fitting:')\n",
    "\n",
    "for t in trees:\n",
    "    for d in depths:\n",
    "        for r in rates:\n",
    "            start_time = datetime.datetime.now()\n",
    "            gb_clf = GradientBoostingClassifier(learning_rate=r, n_estimators=t, verbose=False, random_state=241, max_depth = d)\n",
    "            scores = cross_val_score(gb_clf, X=X, y=y, scoring='roc_auc', cv=kf)\n",
    "            time_to_fit = datetime.datetime.now() - start_time\n",
    "            print('Trees {0}, Depth {1}, Learning Rate {2}, Time to fit: {3}, ROC-AUC: {4}'.format(t, d, r, time_to_fit, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             features  importance\n",
      "6                                 hot    0.002019\n",
      "20                    canceled_16d_nt    0.005088\n",
      "17                           paid_16d    0.005613\n",
      "8                        day_slot_flg    0.008072\n",
      "18                           paid_30d    0.008320\n",
      "4                            cash_flg    0.009337\n",
      "5                            auto_flg    0.010809\n",
      "24                canceled_onetime_nt    0.011134\n",
      "9                         has_comment    0.011813\n",
      "10                   subscription_flg    0.012483\n",
      "29                       dow_canceled    0.013205\n",
      "21                    canceled_30d_nt    0.015207\n",
      "22                    canceled_60d_nt    0.015371\n",
      "7                     additionals_num    0.015371\n",
      "15                    canceled_all_nt    0.016124\n",
      "28                           dow_paid    0.016677\n",
      "3                        discount_prc    0.020242\n",
      "31                 dow_all_paid_share    0.022859\n",
      "19                           paid_60d    0.025745\n",
      "26  days_since_last_cancel_onetime_nt    0.026600\n",
      "25      days_since_last_cancel_all_nt    0.031684\n",
      "14                           paid_all    0.033231\n",
      "30                     dow_paid_share    0.035273\n",
      "16              canceled_all_nt_share    0.039552\n",
      "23                         shifts_num    0.046793\n",
      "1                      subtotal_cents    0.048232\n",
      "2                            discount    0.048256\n",
      "0                         total_cents    0.054417\n",
      "27         days_since_last_paid_order    0.059101\n",
      "11                                lat    0.100180\n",
      "12                                lng    0.108099\n",
      "13               creation_start_hours    0.123092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gapon/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "final_gb_clf = GradientBoostingClassifier(learning_rate=0.2, n_estimators=70, max_depth=5, verbose=False, random_state=241)\n",
    "final_gb_clf.fit(X, y)\n",
    "\n",
    "\n",
    "coeff = final_gb_clf.feature_importances_\n",
    "names = X.columns.values\n",
    "df = pd.DataFrame({'features':names, 'importance':coeff})\n",
    "print(df.sort('importance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('features_imp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['admin', 'auto', 'crm', 'ios', 'reactivation', 'web_new_flow'], dtype=object)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_creation_mean.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump model to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file_name = 'cancellations_model.pickle'\n",
    "#with open(file_name, 'wb') as f:\n",
    "#    pickle.dump(final_gb_clf, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'cancellations_model.pickle'\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'model': final_gb_clf,\n",
    "        #'ohc_dow': ohc_dow,\n",
    "        #'le_type': le_type,\n",
    "        #'ohc_type': ohc_type,\n",
    "        #'le_creation_mean': le_creation_mean,\n",
    "        #'ohc_creation_mean': ohc_creation_mean,\n",
    "        #'le_payment_type': le_payment_type,\n",
    "        #'ohc_payment_type': ohc_payment_type,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест на отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826187226577\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('test/test-20-29-jun.csv', header = 0)\n",
    "\n",
    "X_test = features.drop(['order_id','cancel', 'dow'], axis=1)\n",
    "\n",
    "\n",
    "# X_test = fill_na(X_test, 'shifts_num')\n",
    "\n",
    "X_test = fill_na_mean(X_test, ['shifts_num', 'dow_paid_share'])\n",
    "\n",
    "X_test = X_test.fillna(-999)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X_test = ohc_transform(X_test, 'dow', ohc_dow)\n",
    "\n",
    "#X_test = le_transform(X_test, 'type', le_type)\n",
    "#X_test = ohc_transform(X_test, 'type', ohc_type)\n",
    "\n",
    "#X_test = le_transform(X_test, 'creation_mean', le_creation_mean)\n",
    "#X_test = ohc_transform(X_test, 'creation_mean', ohc_creation_mean)\n",
    "\n",
    "#X_test = le_transform(X_test, 'payment_type', le_payment_type)\n",
    "#X_test = ohc_transform(X_test, 'payment_type', ohc_payment_type)\n",
    "\n",
    "# X_test = exponentiation(\n",
    "#    X_test, ['paid_all','canceled_all','paid_16d','canceled_16d', 'paid_30d','canceled_30d', 'paid_60d','canceled_60d'])\n",
    "\n",
    "\n",
    "y_true = features['cancel']\n",
    "y_pred = final_gb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "print(roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "group_names = ['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '90-100']\n",
    "\n",
    "categories = pd.cut(y_pred, bins, labels=group_names)\n",
    "\n",
    "df = pd.DataFrame(columns=['y_true', 'y_pred', 'bin'])\n",
    "df['y_true'] = y_true\n",
    "df['y_pred'] = y_pred\n",
    "df['bin'] = categories\n",
    "\n",
    "df.to_csv('bins_nt_model-2017-06-29.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47702018  0.50489465  0.47570555 ...,  0.5580376   0.10706268\n",
      "  0.17947012]\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('predict/2017-07-01.csv', header = 0, index_col='order_id')\n",
    "#X_test = read_csv('test/test-2017.csv', index_col=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "X_test = features\n",
    "\n",
    "X_test = features.drop(['dow'], axis=1)\n",
    "X_test = fill_na_mean(X_test, ['shifts_num', 'dow_paid_share'])\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "#y_true = features['cancel']\n",
    "y_pred = final_gb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#print(roc_auc_score(y_true, y_pred))\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index=X_test.index, columns=['will_cancel'])\n",
    "df['will_cancel'] = final_gb_clf.predict_proba(X_test)[:,1]\n",
    "df.to_csv('predict/predict-2017-07-01.csv')\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
